# DigitCV

In this Kaggle challenge we will be working on a project that aims to solve the famous challenge of recognizing handwritten digits using the MNIST dataset. My goal is to create feed forward neural network and train models that can correctly identify and classify handwritten digit images

We started with a simple neural network to classify MNIST handwritten digits and achieved 0.92 accuracy by employing two hidden layers and custom training loops on normalized data. To improve, we used Dropout, a strategy that randomly ignores some neurons during training, which not only kept the accuracy at 0.92 in fewer epochs but also increased it to 0.97 with extended training. Next, we have used the Adam Optimizer, a sophisticated algorithm that optimizes learning rates for each parameter, increasing efficiency and achieving 0.95 accuracy in just one epoch and 0.97 in four epochs and with 20 epochs we reached to 0.98.

We implemented using TensorFlow and Keras and got 0.98 accuracy. However, our initial model suffered from overfitting, performing well on training data but not on new, unknown data. This was demonstrated by the divergent loss curves for training and new data. To address this issue, we created Early Stopping, which halts training when the model stops improving on a validation dataset. This considerably enhanced our model's capacity to generalize, as indicated by increased validation accuracy and a smaller difference between training and validation loss. Each refinement step—Dropout, Adam Optimizer, Early Stopping—was critical in improving our model's accuracy, efficiency, and generalizability, demonstrating the value of these strategies in neural network optimization and training, With a decent accuracy, following modifications and the use of K-Fold cross-validation resulted in a notable final score of 0.98 on Kaggle.
